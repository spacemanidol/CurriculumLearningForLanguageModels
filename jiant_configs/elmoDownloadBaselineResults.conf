// Fast demo config, used mainly for validating that an installation worked and that code
// changes don't crash.
// Run with:
//   python main.py --config_file jiant/config/demo.conf

// This is not meant to yield good performance, but for reference, here are the results
// shown at the end of a typical run with jiant 1.0:

// micro_avg: 0.285, macro_avg: 0.285, sts-b_corr: 0.285, sts-b_pearsonr: 0.297, sts-b_spearmanr: 0.273
// micro_avg: 0.486, macro_avg: 0.486, commitbank_accuracy: 0.696, commitbank_f1: 0.486, commitbank_precision: 0.465, commitbank_recall: 0.508

// Optimization is stochastic, so these numbers will vary slightly across runs.

// This imports the defaults, which can be overridden below.
include "defaults.conf"  // relative path to this file

// write to local storage by default for this demo
exp_name = elmotest
run_name = test1
exp_dir = tmp
run_dir = tmp
project_dir = tmp
local_log_path = log.log
cuda = 1
random_seed = 42

input_module = elmo
elmo_weight_file_path = /home/spacemanidol/ProgressiveLanguageLearning/models/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5

pretrain_tasks = glue
target_tasks = glue

transfer_paradigm = finetune
// Execution, Saving, and Loading //

// Three main stages of operation
do_pretrain = 1  // Train the shared sentence encoder model (and task-specific model parameters) on the pretraining tasks in
              // train_tasks.
do_target_task_training = 1  // After do_train, train the task-specific model parameters on the target tasks in eval_tasks.
do_fulleval = 1     // Evaluate the model on the tasks on eval_tasks.

// Related configuration
allow_untrained_encoder_parameters = 0  // Set for experiments involving random untrained encoders only. Allows train_for_eval
                                        // and do_eval to proceed without pretraining.
