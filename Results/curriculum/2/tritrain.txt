Found 1 shards at wikitext-2/wiki.train.tokens.tri_sort
146778 sentences loaded
Data loaded into memory
Found 1 shards at wikitext-2/wiki.train.tokens.tri_sort
146778 sentences loaded
Data loaded into memory
USING SKIP CONNECTIONS
USING SKIP CONNECTIONS
USING SKIP CONNECTIONS
[['global_step:0', TensorShape([])],
 ['lm/CNN/W_cnn_0:0',
  TensorShape([Dimension(1), Dimension(1), Dimension(16), Dimension(32)])],
 ['lm/CNN/W_cnn_1:0',
  TensorShape([Dimension(1), Dimension(2), Dimension(16), Dimension(32)])],
 ['lm/CNN/W_cnn_2:0',
  TensorShape([Dimension(1), Dimension(3), Dimension(16), Dimension(64)])],
 ['lm/CNN/W_cnn_3:0',
  TensorShape([Dimension(1), Dimension(4), Dimension(16), Dimension(128)])],
 ['lm/CNN/W_cnn_4:0',
  TensorShape([Dimension(1), Dimension(5), Dimension(16), Dimension(256)])],
 ['lm/CNN/W_cnn_5:0',
  TensorShape([Dimension(1), Dimension(6), Dimension(16), Dimension(512)])],
 ['lm/CNN/W_cnn_6:0',
  TensorShape([Dimension(1), Dimension(7), Dimension(16), Dimension(1024)])],
 ['lm/CNN/b_cnn_0:0', TensorShape([Dimension(32)])],
 ['lm/CNN/b_cnn_1:0', TensorShape([Dimension(32)])],
 ['lm/CNN/b_cnn_2:0', TensorShape([Dimension(64)])],
 ['lm/CNN/b_cnn_3:0', TensorShape([Dimension(128)])],
 ['lm/CNN/b_cnn_4:0', TensorShape([Dimension(256)])],
 ['lm/CNN/b_cnn_5:0', TensorShape([Dimension(512)])],
 ['lm/CNN/b_cnn_6:0', TensorShape([Dimension(1024)])],
 ['lm/CNN_high_0/W_carry:0', TensorShape([Dimension(2048), Dimension(2048)])],
 ['lm/CNN_high_0/W_transform:0',
  TensorShape([Dimension(2048), Dimension(2048)])],
 ['lm/CNN_high_0/b_carry:0', TensorShape([Dimension(2048)])],
 ['lm/CNN_high_0/b_transform:0', TensorShape([Dimension(2048)])],
 ['lm/CNN_high_1/W_carry:0', TensorShape([Dimension(2048), Dimension(2048)])],
 ['lm/CNN_high_1/W_transform:0',
  TensorShape([Dimension(2048), Dimension(2048)])],
 ['lm/CNN_high_1/b_carry:0', TensorShape([Dimension(2048)])],
 ['lm/CNN_high_1/b_transform:0', TensorShape([Dimension(2048)])],
 ['lm/CNN_proj/W_proj:0', TensorShape([Dimension(2048), Dimension(512)])],
 ['lm/CNN_proj/b_proj:0', TensorShape([Dimension(512)])],
 ['lm/RNN_0/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0',
  TensorShape([Dimension(16384)])],
 ['lm/RNN_0/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(16384)])],
 ['lm/RNN_0/rnn/multi_rnn_cell/cell_0/lstm_cell/projection/kernel:0',
  TensorShape([Dimension(4096), Dimension(512)])],
 ['lm/RNN_0/rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0',
  TensorShape([Dimension(16384)])],
 ['lm/RNN_0/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(16384)])],
 ['lm/RNN_0/rnn/multi_rnn_cell/cell_1/lstm_cell/projection/kernel:0',
  TensorShape([Dimension(4096), Dimension(512)])],
 ['lm/RNN_1/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0',
  TensorShape([Dimension(16384)])],
 ['lm/RNN_1/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(16384)])],
 ['lm/RNN_1/rnn/multi_rnn_cell/cell_0/lstm_cell/projection/kernel:0',
  TensorShape([Dimension(4096), Dimension(512)])],
 ['lm/RNN_1/rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0',
  TensorShape([Dimension(16384)])],
 ['lm/RNN_1/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(16384)])],
 ['lm/RNN_1/rnn/multi_rnn_cell/cell_1/lstm_cell/projection/kernel:0',
  TensorShape([Dimension(4096), Dimension(512)])],
 ['lm/char_embed:0', TensorShape([Dimension(261), Dimension(16)])],
 ['lm/softmax/W:0', TensorShape([Dimension(33155), Dimension(512)])],
 ['lm/softmax/b:0', TensorShape([Dimension(33155)])],
 ['train_perplexity:0', TensorShape([])]]
Training for 10 epochs and 3823 batches
is Char input:True
Training model with curriculum. Starting competence:0.1. 
 Competence increment 0.0004
Batch 1, train_perplexity=26768.666
Batch 5, train_perplexity=119116.54
Batch 10, train_perplexity=951183700.0
Batch 15, train_perplexity=6660.2744
Batch 20, train_perplexity=124362.27
Batch 25, train_perplexity=3156.8174
Batch 30, train_perplexity=1310.2251
Batch 35, train_perplexity=1746.0056
Batch 40, train_perplexity=985.40955
Batch 45, train_perplexity=911.22974
Batch 50, train_perplexity=816.1475
Batch 55, train_perplexity=564.10657
Batch 60, train_perplexity=709.2141
Batch 65, train_perplexity=594.63715
Batch 70, train_perplexity=496.82538
Batch 75, train_perplexity=699.5168
Batch 80, train_perplexity=467.3507
Batch 85, train_perplexity=403.9339
Batch 90, train_perplexity=660.8305
Batch 95, train_perplexity=483.4942
Batch 100, train_perplexity=433.44025
Batch 105, train_perplexity=548.7285
Batch 110, train_perplexity=498.34555
Batch 115, train_perplexity=431.1674
Batch 120, train_perplexity=489.18042
Batch 125, train_perplexity=458.72455
Batch 130, train_perplexity=349.07077
Batch 135, train_perplexity=363.15952
Batch 140, train_perplexity=467.0201
Batch 145, train_perplexity=410.65976
Batch 150, train_perplexity=350.9882
Batch 155, train_perplexity=380.55957
Batch 160, train_perplexity=339.9423
Batch 165, train_perplexity=373.84027
Batch 170, train_perplexity=318.48978
Batch 175, train_perplexity=327.5439
Batch 180, train_perplexity=292.56268
Batch 185, train_perplexity=327.7778
Batch 190, train_perplexity=305.5878
Batch 195, train_perplexity=283.17868
Batch 200, train_perplexity=291.1342
Batch 205, train_perplexity=336.4978
Batch 210, train_perplexity=307.42978
Batch 215, train_perplexity=317.26126
Batch 220, train_perplexity=276.26782
Batch 225, train_perplexity=313.47653
Batch 230, train_perplexity=289.5008
Batch 235, train_perplexity=282.77563
Batch 240, train_perplexity=284.60046
Batch 245, train_perplexity=318.74106
Batch 250, train_perplexity=269.74948
Batch 255, train_perplexity=283.31915
Batch 260, train_perplexity=279.66876
Batch 265, train_perplexity=267.06332
Batch 270, train_perplexity=269.2217
Batch 275, train_perplexity=226.60841
Batch 280, train_perplexity=278.15198
Batch 285, train_perplexity=237.29297
Batch 290, train_perplexity=239.63168
Batch 295, train_perplexity=299.65347
Batch 300, train_perplexity=221.44223
Batch 305, train_perplexity=248.91971
Batch 310, train_perplexity=242.83253
Batch 315, train_perplexity=261.50064
Batch 320, train_perplexity=229.14365
Batch 325, train_perplexity=243.99042
Batch 330, train_perplexity=225.28839
Batch 335, train_perplexity=228.94301
Batch 340, train_perplexity=225.79903
Batch 345, train_perplexity=247.57959
Batch 350, train_perplexity=203.03896
Batch 355, train_perplexity=228.90338
Batch 360, train_perplexity=213.43672
Batch 365, train_perplexity=228.61497
Batch 370, train_perplexity=218.92456
Batch 375, train_perplexity=222.60541
Batch 380, train_perplexity=207.08176
Batch 385, train_perplexity=204.65445
Batch 390, train_perplexity=208.36763
Batch 395, train_perplexity=195.45749
Batch 400, train_perplexity=209.83731
Batch 405, train_perplexity=236.90257
Batch 410, train_perplexity=188.5036
Batch 415, train_perplexity=181.93857
Batch 420, train_perplexity=188.35338
Batch 425, train_perplexity=186.95944
Batch 430, train_perplexity=174.20729
Batch 435, train_perplexity=202.05325
Batch 440, train_perplexity=195.44714
Batch 445, train_perplexity=199.7023
Batch 450, train_perplexity=334.60352
Batch 455, train_perplexity=189.485
Batch 460, train_perplexity=194.50822
Batch 465, train_perplexity=168.80885
Batch 470, train_perplexity=183.30399
Batch 475, train_perplexity=179.57642
Batch 480, train_perplexity=167.95448
Batch 485, train_perplexity=165.46132
Batch 490, train_perplexity=173.65004
Batch 495, train_perplexity=147.18034
Batch 500, train_perplexity=174.25406
Batch 505, train_perplexity=161.22646
Batch 510, train_perplexity=169.7387
Batch 515, train_perplexity=162.03369
Batch 520, train_perplexity=161.60684
Batch 525, train_perplexity=150.24576
Batch 530, train_perplexity=160.29492
Batch 535, train_perplexity=164.69691
Batch 540, train_perplexity=133.37474
Batch 545, train_perplexity=138.76247
Batch 550, train_perplexity=142.61768
Batch 555, train_perplexity=146.8489
Batch 560, train_perplexity=145.54807
Batch 565, train_perplexity=159.79628
Batch 570, train_perplexity=154.21492
Batch 575, train_perplexity=131.81125
Batch 580, train_perplexity=149.88747
Batch 585, train_perplexity=121.49091
Batch 590, train_perplexity=128.94292
Batch 595, train_perplexity=130.66418
Batch 600, train_perplexity=131.02394
Batch 605, train_perplexity=119.37963
Batch 610, train_perplexity=126.580444
Batch 615, train_perplexity=138.40628
Batch 620, train_perplexity=119.24184
Batch 625, train_perplexity=109.651024
Batch 630, train_perplexity=129.04286
Batch 635, train_perplexity=128.51979
Batch 640, train_perplexity=124.91469
Batch 645, train_perplexity=138.6413
Batch 650, train_perplexity=129.75534
Batch 655, train_perplexity=123.916534
Batch 660, train_perplexity=129.31981
Batch 665, train_perplexity=132.61955
Batch 670, train_perplexity=122.69124
Batch 675, train_perplexity=138.60574
Batch 680, train_perplexity=132.50836
Batch 685, train_perplexity=140.18048
Batch 690, train_perplexity=128.03375
Batch 695, train_perplexity=125.757545
Batch 700, train_perplexity=105.379036
Batch 705, train_perplexity=149.04605
Batch 710, train_perplexity=125.794975
Batch 715, train_perplexity=129.34854
Batch 720, train_perplexity=120.10116
Batch 725, train_perplexity=126.71173
Batch 730, train_perplexity=144.6927
Batch 735, train_perplexity=144.37457
Batch 740, train_perplexity=133.33626
Batch 745, train_perplexity=135.348
Batch 750, train_perplexity=117.90603
Batch 755, train_perplexity=133.53758
Batch 760, train_perplexity=122.88451
Batch 765, train_perplexity=108.21233
Batch 770, train_perplexity=136.6548
Batch 775, train_perplexity=119.466354
Batch 780, train_perplexity=119.23734
Batch 785, train_perplexity=126.82743
Batch 790, train_perplexity=113.17057
Batch 795, train_perplexity=117.526855
Batch 800, train_perplexity=124.94715
Batch 805, train_perplexity=123.41924
Batch 810, train_perplexity=110.77329
Batch 815, train_perplexity=133.55095
Batch 820, train_perplexity=117.81189
Batch 825, train_perplexity=131.11543
Batch 830, train_perplexity=122.03983
Batch 835, train_perplexity=123.99804
Batch 840, train_perplexity=113.73622
Batch 845, train_perplexity=118.74507
Batch 850, train_perplexity=119.63783
Batch 855, train_perplexity=112.594246
Batch 860, train_perplexity=111.48367
Batch 865, train_perplexity=119.78888
Batch 870, train_perplexity=107.1851
Batch 875, train_perplexity=111.454865
Batch 880, train_perplexity=112.67492
Batch 885, train_perplexity=110.499596
Batch 890, train_perplexity=113.74219
Batch 895, train_perplexity=101.73353
Batch 900, train_perplexity=94.908
Batch 905, train_perplexity=119.47154
Batch 910, train_perplexity=96.53814
Batch 915, train_perplexity=100.26419
Batch 920, train_perplexity=106.01579
Batch 925, train_perplexity=89.133286
Batch 930, train_perplexity=103.59135
Batch 935, train_perplexity=97.65864
Batch 940, train_perplexity=96.66975
Batch 945, train_perplexity=94.1063
Batch 950, train_perplexity=95.58511
Batch 955, train_perplexity=85.581825
Batch 960, train_perplexity=105.71765
Batch 965, train_perplexity=88.287704
Batch 970, train_perplexity=86.08607
Batch 975, train_perplexity=95.21228
Batch 980, train_perplexity=77.97331
Batch 985, train_perplexity=83.0343
Batch 990, train_perplexity=82.38742
Batch 995, train_perplexity=89.853096
Batch 1000, train_perplexity=72.3269
Batch 1005, train_perplexity=74.66818
Batch 1010, train_perplexity=87.875046
Batch 1015, train_perplexity=67.94004
Batch 1020, train_perplexity=78.96711
Batch 1025, train_perplexity=72.4264
Batch 1030, train_perplexity=74.31763
Batch 1035, train_perplexity=79.15655
Batch 1040, train_perplexity=70.17598
Batch 1045, train_perplexity=61.417973
Batch 1050, train_perplexity=65.86944
Batch 1055, train_perplexity=70.06438
Batch 1060, train_perplexity=63.179832
Batch 1065, train_perplexity=65.362785
Batch 1070, train_perplexity=57.43221
Batch 1075, train_perplexity=70.640396
Batch 1080, train_perplexity=56.793102
Batch 1085, train_perplexity=59.019672
Batch 1090, train_perplexity=63.34182
Batch 1095, train_perplexity=62.77892
Batch 1100, train_perplexity=65.555626
Batch 1105, train_perplexity=60.20796
Batch 1110, train_perplexity=55.7868
Batch 1115, train_perplexity=60.53816
Batch 1120, train_perplexity=52.41611
Batch 1125, train_perplexity=65.85226
Batch 1130, train_perplexity=57.12108
Batch 1135, train_perplexity=58.11383
Batch 1140, train_perplexity=53.817787
Batch 1145, train_perplexity=52.902092
Batch 1150, train_perplexity=57.120094
Batch 1155, train_perplexity=57.96043
Batch 1160, train_perplexity=54.079628
Batch 1165, train_perplexity=61.58563
Batch 1170, train_perplexity=60.92168
Batch 1175, train_perplexity=52.404377
Batch 1180, train_perplexity=54.39167
Batch 1185, train_perplexity=56.802204
Batch 1190, train_perplexity=55.12543
Batch 1195, train_perplexity=55.501675
Batch 1200, train_perplexity=60.297657
Batch 1205, train_perplexity=50.22742
Batch 1210, train_perplexity=56.60644
Batch 1215, train_perplexity=56.881645
Batch 1220, train_perplexity=64.84495
Batch 1225, train_perplexity=57.097794
Batch 1230, train_perplexity=49.06353
Batch 1235, train_perplexity=59.025867
Batch 1240, train_perplexity=51.671703
Batch 1245, train_perplexity=58.661385
Batch 1250, train_perplexity=54.618668
Batch 1255, train_perplexity=56.88425
Batch 1260, train_perplexity=58.555214
Batch 1265, train_perplexity=54.158348
Batch 1270, train_perplexity=59.262142
Batch 1275, train_perplexity=53.09855
Batch 1280, train_perplexity=53.770103
Batch 1285, train_perplexity=57.682236
Batch 1290, train_perplexity=53.999863
Batch 1295, train_perplexity=58.5285
Batch 1300, train_perplexity=52.443283
Batch 1305, train_perplexity=55.72193
Batch 1310, train_perplexity=60.947655
Batch 1315, train_perplexity=52.643673
Batch 1320, train_perplexity=58.754013
Batch 1325, train_perplexity=51.712498
Batch 1330, train_perplexity=54.505444
Batch 1335, train_perplexity=66.3316
Batch 1340, train_perplexity=54.064404
Batch 1345, train_perplexity=53.649605
Batch 1350, train_perplexity=57.503704
Batch 1355, train_perplexity=54.923008
Batch 1360, train_perplexity=59.114704
Batch 1365, train_perplexity=51.694794
Batch 1370, train_perplexity=52.948593
Batch 1375, train_perplexity=52.59088
Batch 1380, train_perplexity=54.66799
Batch 1385, train_perplexity=56.55119
Batch 1390, train_perplexity=50.579124
Batch 1395, train_perplexity=51.763824
Batch 1400, train_perplexity=53.735832
Batch 1405, train_perplexity=54.769936
Batch 1410, train_perplexity=59.15297
Batch 1415, train_perplexity=51.819008
Batch 1420, train_perplexity=56.89857
Batch 1425, train_perplexity=53.9795
Batch 1430, train_perplexity=50.612755
Batch 1435, train_perplexity=54.9822
Batch 1440, train_perplexity=64.38958
Batch 1445, train_perplexity=63.00558
Batch 1450, train_perplexity=53.453518
Batch 1455, train_perplexity=50.956245
Batch 1460, train_perplexity=47.81231
Batch 1465, train_perplexity=50.855072
Batch 1470, train_perplexity=49.65898
Batch 1475, train_perplexity=48.56853
Batch 1480, train_perplexity=49.6151
Batch 1485, train_perplexity=49.358635
Batch 1490, train_perplexity=52.35738
Batch 1495, train_perplexity=53.974453
Batch 1500, train_perplexity=46.36013
Batch 1505, train_perplexity=44.109776
Batch 1510, train_perplexity=50.836563
Batch 1515, train_perplexity=56.219677
Batch 1520, train_perplexity=54.703247
Batch 1525, train_perplexity=54.659966
Batch 1530, train_perplexity=48.543377
Batch 1535, train_perplexity=52.218506
Batch 1540, train_perplexity=58.33285
Batch 1545, train_perplexity=57.453823
Batch 1550, train_perplexity=52.047016
Batch 1555, train_perplexity=47.208298
Batch 1560, train_perplexity=56.866077
Batch 1565, train_perplexity=53.197792
Batch 1570, train_perplexity=49.348137
Batch 1575, train_perplexity=49.536922
Batch 1580, train_perplexity=47.411057
Batch 1585, train_perplexity=49.28971
Batch 1590, train_perplexity=48.734623
Batch 1595, train_perplexity=50.843
Batch 1600, train_perplexity=48.513813
Batch 1605, train_perplexity=48.254677
Batch 1610, train_perplexity=52.862442
Batch 1615, train_perplexity=49.54302
Batch 1620, train_perplexity=53.840748
Batch 1625, train_perplexity=44.41217
Batch 1630, train_perplexity=48.62505
Batch 1635, train_perplexity=47.231705
Batch 1640, train_perplexity=51.539955
Batch 1645, train_perplexity=46.68709
Batch 1650, train_perplexity=51.89279
Batch 1655, train_perplexity=44.3107
Batch 1660, train_perplexity=49.27293
Batch 1665, train_perplexity=49.693184
Batch 1670, train_perplexity=47.98785
Batch 1675, train_perplexity=47.547745
Batch 1680, train_perplexity=47.862846
Batch 1685, train_perplexity=47.634888
Batch 1690, train_perplexity=52.42668
Batch 1695, train_perplexity=50.43027
Batch 1700, train_perplexity=44.31051
Batch 1705, train_perplexity=51.27036
Batch 1710, train_perplexity=44.913986
Batch 1715, train_perplexity=45.483307
Batch 1720, train_perplexity=42.71146
Batch 1725, train_perplexity=45.437115
Batch 1730, train_perplexity=51.155914
Batch 1735, train_perplexity=46.383713
Batch 1740, train_perplexity=44.3925
Batch 1745, train_perplexity=49.00093
Batch 1750, train_perplexity=44.841763
Batch 1755, train_perplexity=45.334393
Batch 1760, train_perplexity=44.17768
Batch 1765, train_perplexity=49.52946
Batch 1770, train_perplexity=48.957058
Batch 1775, train_perplexity=46.573902
Batch 1780, train_perplexity=40.455097
Batch 1785, train_perplexity=43.64092
Batch 1790, train_perplexity=49.044537
Batch 1795, train_perplexity=54.373768
Batch 1800, train_perplexity=56.206596
Batch 1805, train_perplexity=51.99313
Batch 1810, train_perplexity=43.646538
Batch 1815, train_perplexity=53.43015
Batch 1820, train_perplexity=46.223076
Batch 1825, train_perplexity=44.230965
Batch 1830, train_perplexity=45.707092
Batch 1835, train_perplexity=47.649338
Batch 1840, train_perplexity=50.585804
Batch 1845, train_perplexity=48.06806
Batch 1850, train_perplexity=51.78476
Batch 1855, train_perplexity=52.13993
Batch 1860, train_perplexity=47.422226
Batch 1865, train_perplexity=47.488686
Batch 1870, train_perplexity=52.539185
Batch 1875, train_perplexity=49.547752
Batch 1880, train_perplexity=48.265743
Batch 1885, train_perplexity=46.50667
Batch 1890, train_perplexity=46.662666
Batch 1895, train_perplexity=50.97864
Batch 1900, train_perplexity=45.22442
Batch 1905, train_perplexity=54.51147
Batch 1910, train_perplexity=49.355762
Batch 1915, train_perplexity=48.699127
Batch 1920, train_perplexity=46.516132
Batch 1925, train_perplexity=47.93749
Batch 1930, train_perplexity=47.504642
Batch 1935, train_perplexity=51.09214
Batch 1940, train_perplexity=47.5408
Batch 1945, train_perplexity=49.812733
Batch 1950, train_perplexity=47.087727
Batch 1955, train_perplexity=49.418076
Batch 1960, train_perplexity=57.229202
Batch 1965, train_perplexity=52.191746
Batch 1970, train_perplexity=51.068493
Batch 1975, train_perplexity=52.96152
Batch 1980, train_perplexity=55.19263
Batch 1985, train_perplexity=50.02806
Batch 1990, train_perplexity=44.576534
Batch 1995, train_perplexity=45.24149
Batch 2000, train_perplexity=48.378418
Batch 2005, train_perplexity=50.295017
Batch 2010, train_perplexity=46.733295
Batch 2015, train_perplexity=48.98966
Batch 2020, train_perplexity=51.017414
Batch 2025, train_perplexity=40.082962
Batch 2030, train_perplexity=46.77298
Batch 2035, train_perplexity=56.20858
Batch 2040, train_perplexity=49.086758
Batch 2045, train_perplexity=48.005527
Batch 2050, train_perplexity=51.15744
Batch 2055, train_perplexity=44.404716
Batch 2060, train_perplexity=51.270557
Batch 2065, train_perplexity=45.204735
Batch 2070, train_perplexity=48.03675
Batch 2075, train_perplexity=44.417316
Batch 2080, train_perplexity=48.946075
Batch 2085, train_perplexity=49.978657
Batch 2090, train_perplexity=52.220287
Batch 2095, train_perplexity=53.902645
Batch 2100, train_perplexity=51.677345
Batch 2105, train_perplexity=52.656452
Batch 2110, train_perplexity=47.604313
Batch 2115, train_perplexity=46.624233
Batch 2120, train_perplexity=47.840427
Batch 2125, train_perplexity=46.474926
Batch 2130, train_perplexity=43.107025
Batch 2135, train_perplexity=49.820263
Batch 2140, train_perplexity=55.077927
Batch 2145, train_perplexity=55.411766
Batch 2150, train_perplexity=51.789104
Batch 2155, train_perplexity=50.678417
Batch 2160, train_perplexity=43.303783
Batch 2165, train_perplexity=46.7257
Batch 2170, train_perplexity=44.124226
Batch 2175, train_perplexity=50.961334
Batch 2180, train_perplexity=46.362263
Batch 2185, train_perplexity=51.311314
Batch 2190, train_perplexity=45.06336
Batch 2195, train_perplexity=48.53832
Batch 2200, train_perplexity=49.80025
Batch 2205, train_perplexity=48.50664
Batch 2210, train_perplexity=48.187973
Batch 2215, train_perplexity=48.495483
Batch 2220, train_perplexity=45.841743
Batch 2225, train_perplexity=45.069664
Batch 2230, train_perplexity=47.618027
Batch 2235, train_perplexity=52.13911
Batch 2240, train_perplexity=49.462337
Batch 2245, train_perplexity=43.636646
Batch 2250, train_perplexity=48.745583
Batch 2255, train_perplexity=46.55066
Batch 2260, train_perplexity=52.62038
Batch 2265, train_perplexity=45.643017
Batch 2270, train_perplexity=48.27469
Batch 2275, train_perplexity=43.60609
Batch 2280, train_perplexity=44.471992
Batch 2285, train_perplexity=48.454338
Batch 2290, train_perplexity=52.007374
Batch 2295, train_perplexity=53.319157
Batch 2300, train_perplexity=46.09523
Batch 2305, train_perplexity=51.26309
Batch 2310, train_perplexity=44.428352
Batch 2315, train_perplexity=52.33715
Batch 2320, train_perplexity=40.831802
Batch 2325, train_perplexity=51.03552
Batch 2330, train_perplexity=47.51674
Batch 2335, train_perplexity=48.46243
Batch 2340, train_perplexity=50.080284
Batch 2345, train_perplexity=45.96585
Batch 2350, train_perplexity=47.7907
Batch 2355, train_perplexity=45.297497
Batch 2360, train_perplexity=48.248867
Batch 2365, train_perplexity=41.489872
Batch 2370, train_perplexity=48.724365
Batch 2375, train_perplexity=52.547565
Batch 2380, train_perplexity=50.341087
Batch 2385, train_perplexity=51.34591
Batch 2390, train_perplexity=45.60315
Batch 2395, train_perplexity=45.45123
Batch 2400, train_perplexity=49.44845
Batch 2405, train_perplexity=50.63156
Batch 2410, train_perplexity=48.85361
Batch 2415, train_perplexity=52.928196
Batch 2420, train_perplexity=44.503304
Batch 2425, train_perplexity=43.977947
Batch 2430, train_perplexity=46.301853
Batch 2435, train_perplexity=42.25647
Batch 2440, train_perplexity=40.866695
Batch 2445, train_perplexity=43.90369
Batch 2450, train_perplexity=42.63987
Batch 2455, train_perplexity=49.60268
Batch 2460, train_perplexity=44.899822
Batch 2465, train_perplexity=45.127544
Batch 2470, train_perplexity=49.91128
Batch 2475, train_perplexity=45.183582
Batch 2480, train_perplexity=44.09375
Batch 2485, train_perplexity=46.800087
Batch 2490, train_perplexity=45.799484
Batch 2495, train_perplexity=41.537083
Batch 2500, train_perplexity=48.44712
Batch 2505, train_perplexity=42.005512
Batch 2510, train_perplexity=50.28335
Batch 2515, train_perplexity=39.6145
Batch 2520, train_perplexity=46.06763
Batch 2525, train_perplexity=49.670444
Batch 2530, train_perplexity=44.913868
Batch 2535, train_perplexity=58.662136
Batch 2540, train_perplexity=41.44373
Batch 2545, train_perplexity=49.268234
Batch 2550, train_perplexity=40.438396
Batch 2555, train_perplexity=43.62483
Batch 2560, train_perplexity=39.481117
Batch 2565, train_perplexity=50.314774
Batch 2570, train_perplexity=46.62956
Batch 2575, train_perplexity=43.257843
Batch 2580, train_perplexity=44.90556
Batch 2585, train_perplexity=43.816322
Batch 2590, train_perplexity=39.86288
Batch 2595, train_perplexity=42.284912
Batch 2600, train_perplexity=44.00829
Batch 2605, train_perplexity=46.18984
Batch 2610, train_perplexity=43.641388
Batch 2615, train_perplexity=46.409145
Batch 2620, train_perplexity=45.715015
Batch 2625, train_perplexity=41.72585
Batch 2630, train_perplexity=45.54948
Batch 2635, train_perplexity=40.318848
Batch 2640, train_perplexity=47.927376
Batch 2645, train_perplexity=44.509724
Batch 2650, train_perplexity=43.342175
Batch 2655, train_perplexity=48.98397
Batch 2660, train_perplexity=42.391506
Batch 2665, train_perplexity=44.59799
Batch 2670, train_perplexity=42.04595
Batch 2675, train_perplexity=44.287823
Batch 2680, train_perplexity=47.251266
Batch 2685, train_perplexity=44.604485
Batch 2690, train_perplexity=42.242832
Batch 2695, train_perplexity=46.24877
Batch 2700, train_perplexity=48.27235
Batch 2705, train_perplexity=48.090702
Batch 2710, train_perplexity=39.655945
Batch 2715, train_perplexity=42.956547
Batch 2720, train_perplexity=43.260883
Batch 2725, train_perplexity=48.05273
Batch 2730, train_perplexity=41.101807
Batch 2735, train_perplexity=42.980858
Batch 2740, train_perplexity=39.19454
Batch 2745, train_perplexity=43.386818
Batch 2750, train_perplexity=43.47871
Batch 2755, train_perplexity=44.198612
Batch 2760, train_perplexity=43.677486
Batch 2765, train_perplexity=41.762566
Batch 2770, train_perplexity=38.835728
Batch 2775, train_perplexity=47.177155
Batch 2780, train_perplexity=41.87607
Batch 2785, train_perplexity=45.214565
Batch 2790, train_perplexity=45.825634
Batch 2795, train_perplexity=42.82606
Batch 2800, train_perplexity=40.29271
Batch 2805, train_perplexity=42.187393
Batch 2810, train_perplexity=37.690636
Batch 2815, train_perplexity=39.380142
Batch 2820, train_perplexity=40.929142
Batch 2825, train_perplexity=42.728653
Batch 2830, train_perplexity=45.0996
Batch 2835, train_perplexity=43.999657
Batch 2840, train_perplexity=37.11319
Batch 2845, train_perplexity=43.45623
Batch 2850, train_perplexity=45.489044
Batch 2855, train_perplexity=43.02277
Batch 2860, train_perplexity=42.449398
Batch 2865, train_perplexity=43.294647
Batch 2870, train_perplexity=47.017838
Batch 2875, train_perplexity=42.45735
Batch 2880, train_perplexity=36.479187
Batch 2885, train_perplexity=41.15718
Batch 2890, train_perplexity=43.382248
Batch 2895, train_perplexity=38.126785
Batch 2900, train_perplexity=42.059265
Batch 2905, train_perplexity=42.16299
Batch 2910, train_perplexity=42.393993
Batch 2915, train_perplexity=36.94352
Batch 2920, train_perplexity=40.87658
Batch 2925, train_perplexity=41.893124
Batch 2930, train_perplexity=41.544994
Batch 2935, train_perplexity=40.21998
Batch 2940, train_perplexity=40.280243
Batch 2945, train_perplexity=38.51611
Batch 2950, train_perplexity=42.823887
Batch 2955, train_perplexity=39.414246
Batch 2960, train_perplexity=46.131367
Batch 2965, train_perplexity=38.87773
Batch 2970, train_perplexity=34.368317
Batch 2975, train_perplexity=35.831963
Batch 2980, train_perplexity=37.649403
Batch 2985, train_perplexity=39.664135
Batch 2990, train_perplexity=44.559757
Batch 2995, train_perplexity=42.202534
Batch 3000, train_perplexity=44.96548
Batch 3005, train_perplexity=44.66809
Batch 3010, train_perplexity=39.29326
Batch 3015, train_perplexity=44.06939
Batch 3020, train_perplexity=41.387577
Batch 3025, train_perplexity=40.039474
Batch 3030, train_perplexity=42.21311
Batch 3035, train_perplexity=39.833447
Batch 3040, train_perplexity=42.851257
Batch 3045, train_perplexity=39.737377
Batch 3050, train_perplexity=41.17899
Batch 3055, train_perplexity=44.339718
Batch 3060, train_perplexity=39.01738
Batch 3065, train_perplexity=39.552097
Batch 3070, train_perplexity=40.54542
Batch 3075, train_perplexity=38.202145
Batch 3080, train_perplexity=37.932644
Batch 3085, train_perplexity=39.167877
Batch 3090, train_perplexity=39.6515
Batch 3095, train_perplexity=39.88484
Batch 3100, train_perplexity=36.186813
Batch 3105, train_perplexity=36.55809
Batch 3110, train_perplexity=37.560482
Batch 3115, train_perplexity=40.866814
Batch 3120, train_perplexity=39.002415
Batch 3125, train_perplexity=39.002323
Batch 3130, train_perplexity=40.186447
Batch 3135, train_perplexity=46.16552
Batch 3140, train_perplexity=40.299915
Batch 3145, train_perplexity=38.47409
Batch 3150, train_perplexity=36.772415
Batch 3155, train_perplexity=38.71847
Batch 3160, train_perplexity=39.186558
Batch 3165, train_perplexity=42.309757
Batch 3170, train_perplexity=37.79411
Batch 3175, train_perplexity=45.82194
Batch 3180, train_perplexity=41.068394
Batch 3185, train_perplexity=39.501106
Batch 3190, train_perplexity=34.400772
Batch 3195, train_perplexity=41.95701
Batch 3200, train_perplexity=38.331844
Batch 3205, train_perplexity=40.644463
Batch 3210, train_perplexity=42.33786
Batch 3215, train_perplexity=41.205143
Batch 3220, train_perplexity=40.70886
Batch 3225, train_perplexity=39.657722
Batch 3230, train_perplexity=40.34435
Batch 3235, train_perplexity=37.618504
Batch 3240, train_perplexity=33.51588
Batch 3245, train_perplexity=37.81569
Batch 3250, train_perplexity=38.76616
Batch 3255, train_perplexity=36.309906
Batch 3260, train_perplexity=33.540165
Batch 3265, train_perplexity=39.528427
Batch 3270, train_perplexity=41.378223
Batch 3275, train_perplexity=35.626675
Batch 3280, train_perplexity=39.591873
Batch 3285, train_perplexity=38.708427
Batch 3290, train_perplexity=40.04525
Batch 3295, train_perplexity=39.492073
Batch 3300, train_perplexity=48.37143
Batch 3305, train_perplexity=38.996624
Batch 3310, train_perplexity=46.57324
Batch 3315, train_perplexity=35.936817
Batch 3320, train_perplexity=34.371277
Batch 3325, train_perplexity=35.003742
Batch 3330, train_perplexity=34.43469
Batch 3335, train_perplexity=35.3414
Batch 3340, train_perplexity=36.045616
Batch 3345, train_perplexity=37.160595
Batch 3350, train_perplexity=50.25465
Batch 3355, train_perplexity=37.78654
Batch 3360, train_perplexity=40.541225
Batch 3365, train_perplexity=40.04994
Batch 3370, train_perplexity=35.846188
Batch 3375, train_perplexity=35.785492
Batch 3380, train_perplexity=37.515377
Batch 3385, train_perplexity=38.148453
Batch 3390, train_perplexity=42.724026
Batch 3395, train_perplexity=38.559826
Batch 3400, train_perplexity=41.19983
Batch 3405, train_perplexity=39.618145
Batch 3410, train_perplexity=35.46983
Batch 3415, train_perplexity=33.70757
Batch 3420, train_perplexity=37.466534
Batch 3425, train_perplexity=42.41662
Batch 3430, train_perplexity=39.360542
Batch 3435, train_perplexity=33.38683
Batch 3440, train_perplexity=34.440205
Batch 3445, train_perplexity=34.626106
Batch 3450, train_perplexity=37.166645
Batch 3455, train_perplexity=38.647537
Batch 3460, train_perplexity=37.37953
Batch 3465, train_perplexity=34.419067
Batch 3470, train_perplexity=42.503616
Batch 3475, train_perplexity=36.4038
Batch 3480, train_perplexity=34.828526
Batch 3485, train_perplexity=37.37211
Batch 3490, train_perplexity=36.559116
Batch 3495, train_perplexity=37.647167
Batch 3500, train_perplexity=35.019375
Batch 3505, train_perplexity=37.930508
Batch 3510, train_perplexity=37.279682
Batch 3515, train_perplexity=32.89035
Batch 3520, train_perplexity=38.18497
Batch 3525, train_perplexity=34.037884
Batch 3530, train_perplexity=35.284573
Batch 3535, train_perplexity=37.864506
Batch 3540, train_perplexity=39.15866
Batch 3545, train_perplexity=34.127415
Batch 3550, train_perplexity=34.194138
Batch 3555, train_perplexity=33.810383
Batch 3560, train_perplexity=36.61333
Batch 3565, train_perplexity=34.948105
Batch 3570, train_perplexity=31.756887
Batch 3575, train_perplexity=34.151688
Batch 3580, train_perplexity=34.290062
Batch 3585, train_perplexity=37.234055
Batch 3590, train_perplexity=36.37627
Batch 3595, train_perplexity=35.28792
Batch 3600, train_perplexity=35.182392
Batch 3605, train_perplexity=35.282585
Batch 3610, train_perplexity=38.27515
Batch 3615, train_perplexity=38.268345
Batch 3620, train_perplexity=34.62183
Batch 3625, train_perplexity=35.597866
Batch 3630, train_perplexity=34.72616
Batch 3635, train_perplexity=37.244705
Batch 3640, train_perplexity=40.480686
Batch 3645, train_perplexity=36.50555
Batch 3650, train_perplexity=35.34089
Batch 3655, train_perplexity=35.553955
Batch 3660, train_perplexity=34.407673
Batch 3665, train_perplexity=32.751087
Batch 3670, train_perplexity=35.966946
Batch 3675, train_perplexity=36.744293
Batch 3680, train_perplexity=34.00596
Batch 3685, train_perplexity=39.44953
Batch 3690, train_perplexity=37.164627
Batch 3695, train_perplexity=36.434338
Batch 3700, train_perplexity=31.64035
Batch 3705, train_perplexity=38.743633
Batch 3710, train_perplexity=34.81036
Batch 3715, train_perplexity=30.59776
Batch 3720, train_perplexity=35.47991
Batch 3725, train_perplexity=32.750065
Batch 3730, train_perplexity=34.43738
Batch 3735, train_perplexity=36.336155
Batch 3740, train_perplexity=33.765995
Batch 3745, train_perplexity=35.47284
Batch 3750, train_perplexity=36.00758
Batch 3755, train_perplexity=34.91337
Batch 3760, train_perplexity=34.838192
Batch 3765, train_perplexity=33.995907
Batch 3770, train_perplexity=31.327204
Batch 3775, train_perplexity=36.73407
Batch 3780, train_perplexity=34.98483
Batch 3785, train_perplexity=36.13054
Batch 3790, train_perplexity=37.27356
Batch 3795, train_perplexity=35.62919
Batch 3800, train_perplexity=35.462505
Batch 3805, train_perplexity=31.82996
Batch 3810, train_perplexity=31.481857
Batch 3815, train_perplexity=31.524345
Batch 3820, train_perplexity=30.26838
Done training
