Found 1 shards at wikitext-2/wiki.train.tokens.len_sort
146778 sentences loaded
Data loaded into memory
Found 1 shards at wikitext-2/wiki.train.tokens.len_sort
146778 sentences loaded
Data loaded into memory
USING SKIP CONNECTIONS
USING SKIP CONNECTIONS
USING SKIP CONNECTIONS
[['global_step:0', TensorShape([])],
 ['lm/CNN/W_cnn_0:0',
  TensorShape([Dimension(1), Dimension(1), Dimension(16), Dimension(32)])],
 ['lm/CNN/W_cnn_1:0',
  TensorShape([Dimension(1), Dimension(2), Dimension(16), Dimension(32)])],
 ['lm/CNN/W_cnn_2:0',
  TensorShape([Dimension(1), Dimension(3), Dimension(16), Dimension(64)])],
 ['lm/CNN/W_cnn_3:0',
  TensorShape([Dimension(1), Dimension(4), Dimension(16), Dimension(128)])],
 ['lm/CNN/W_cnn_4:0',
  TensorShape([Dimension(1), Dimension(5), Dimension(16), Dimension(256)])],
 ['lm/CNN/W_cnn_5:0',
  TensorShape([Dimension(1), Dimension(6), Dimension(16), Dimension(512)])],
 ['lm/CNN/W_cnn_6:0',
  TensorShape([Dimension(1), Dimension(7), Dimension(16), Dimension(1024)])],
 ['lm/CNN/b_cnn_0:0', TensorShape([Dimension(32)])],
 ['lm/CNN/b_cnn_1:0', TensorShape([Dimension(32)])],
 ['lm/CNN/b_cnn_2:0', TensorShape([Dimension(64)])],
 ['lm/CNN/b_cnn_3:0', TensorShape([Dimension(128)])],
 ['lm/CNN/b_cnn_4:0', TensorShape([Dimension(256)])],
 ['lm/CNN/b_cnn_5:0', TensorShape([Dimension(512)])],
 ['lm/CNN/b_cnn_6:0', TensorShape([Dimension(1024)])],
 ['lm/CNN_high_0/W_carry:0', TensorShape([Dimension(2048), Dimension(2048)])],
 ['lm/CNN_high_0/W_transform:0',
  TensorShape([Dimension(2048), Dimension(2048)])],
 ['lm/CNN_high_0/b_carry:0', TensorShape([Dimension(2048)])],
 ['lm/CNN_high_0/b_transform:0', TensorShape([Dimension(2048)])],
 ['lm/CNN_high_1/W_carry:0', TensorShape([Dimension(2048), Dimension(2048)])],
 ['lm/CNN_high_1/W_transform:0',
  TensorShape([Dimension(2048), Dimension(2048)])],
 ['lm/CNN_high_1/b_carry:0', TensorShape([Dimension(2048)])],
 ['lm/CNN_high_1/b_transform:0', TensorShape([Dimension(2048)])],
 ['lm/CNN_proj/W_proj:0', TensorShape([Dimension(2048), Dimension(512)])],
 ['lm/CNN_proj/b_proj:0', TensorShape([Dimension(512)])],
 ['lm/RNN_0/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0',
  TensorShape([Dimension(16384)])],
 ['lm/RNN_0/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(16384)])],
 ['lm/RNN_0/rnn/multi_rnn_cell/cell_0/lstm_cell/projection/kernel:0',
  TensorShape([Dimension(4096), Dimension(512)])],
 ['lm/RNN_0/rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0',
  TensorShape([Dimension(16384)])],
 ['lm/RNN_0/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(16384)])],
 ['lm/RNN_0/rnn/multi_rnn_cell/cell_1/lstm_cell/projection/kernel:0',
  TensorShape([Dimension(4096), Dimension(512)])],
 ['lm/RNN_1/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0',
  TensorShape([Dimension(16384)])],
 ['lm/RNN_1/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(16384)])],
 ['lm/RNN_1/rnn/multi_rnn_cell/cell_0/lstm_cell/projection/kernel:0',
  TensorShape([Dimension(4096), Dimension(512)])],
 ['lm/RNN_1/rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0',
  TensorShape([Dimension(16384)])],
 ['lm/RNN_1/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(16384)])],
 ['lm/RNN_1/rnn/multi_rnn_cell/cell_1/lstm_cell/projection/kernel:0',
  TensorShape([Dimension(4096), Dimension(512)])],
 ['lm/char_embed:0', TensorShape([Dimension(261), Dimension(16)])],
 ['lm/softmax/W:0', TensorShape([Dimension(33155), Dimension(512)])],
 ['lm/softmax/b:0', TensorShape([Dimension(33155)])],
 ['train_perplexity:0', TensorShape([])]]
Training for 10 epochs and 3823 batches
is Char input:True
Training model with curriculum. Starting competence:0.1. 
 Competence increment 0.0004
Batch 1, train_perplexity=30703.277
Batch 5, train_perplexity=845.50574
Batch 10, train_perplexity=165495.58
Batch 15, train_perplexity=113.31555
Batch 20, train_perplexity=146.16574
Batch 25, train_perplexity=13336.559
Batch 30, train_perplexity=548.7945
Batch 35, train_perplexity=297.89514
Batch 40, train_perplexity=87.106636
Batch 45, train_perplexity=35.630775
Batch 50, train_perplexity=118.399956
Batch 55, train_perplexity=23.633015
Batch 60, train_perplexity=78.70896
Batch 65, train_perplexity=28.080286
Batch 70, train_perplexity=44.33307
Batch 75, train_perplexity=29.403502
Batch 80, train_perplexity=126.69179
Batch 85, train_perplexity=26.649265
Batch 90, train_perplexity=39.171127
Batch 95, train_perplexity=27.517717
Batch 100, train_perplexity=29.893839
Batch 105, train_perplexity=30.160036
Batch 110, train_perplexity=22.180305
Batch 115, train_perplexity=25.393549
Batch 120, train_perplexity=23.387762
Batch 125, train_perplexity=27.277176
Batch 130, train_perplexity=38.488697
Batch 135, train_perplexity=43.734196
Batch 140, train_perplexity=27.78172
Batch 145, train_perplexity=36.1647
Batch 150, train_perplexity=22.725496
Batch 155, train_perplexity=26.02426
Batch 160, train_perplexity=27.235716
Batch 165, train_perplexity=29.076658
Batch 170, train_perplexity=42.443264
Batch 175, train_perplexity=27.39874
Batch 180, train_perplexity=28.23622
Batch 185, train_perplexity=27.20055
Batch 190, train_perplexity=31.099514
Batch 195, train_perplexity=27.562681
Batch 200, train_perplexity=30.010983
Batch 205, train_perplexity=28.430883
Batch 210, train_perplexity=28.26659
Batch 215, train_perplexity=29.801874
Batch 220, train_perplexity=30.93822
Batch 225, train_perplexity=36.751442
Batch 230, train_perplexity=30.447916
Batch 235, train_perplexity=33.470028
Batch 240, train_perplexity=31.340187
Batch 245, train_perplexity=36.943424
Batch 250, train_perplexity=31.807087
Batch 255, train_perplexity=40.55134
Batch 260, train_perplexity=34.67049
Batch 265, train_perplexity=34.824913
Batch 270, train_perplexity=38.812122
Batch 275, train_perplexity=36.02854
Batch 280, train_perplexity=47.736393
Batch 285, train_perplexity=37.923626
Batch 290, train_perplexity=42.841125
Batch 295, train_perplexity=39.432556
Batch 300, train_perplexity=39.99738
Batch 305, train_perplexity=41.03401
Batch 310, train_perplexity=37.19331
Batch 315, train_perplexity=41.34347
Batch 320, train_perplexity=42.033844
Batch 325, train_perplexity=46.812016
Batch 330, train_perplexity=45.384224
Batch 335, train_perplexity=42.39397
Batch 340, train_perplexity=41.33009
Batch 345, train_perplexity=41.165756
Batch 350, train_perplexity=47.10696
Batch 355, train_perplexity=59.601986
Batch 360, train_perplexity=47.58821
Batch 365, train_perplexity=49.869747
Batch 370, train_perplexity=48.347565
Batch 375, train_perplexity=47.772953
Batch 380, train_perplexity=53.836845
Batch 385, train_perplexity=55.240025
Batch 390, train_perplexity=56.95289
Batch 395, train_perplexity=56.204666
Batch 400, train_perplexity=56.399952
Batch 405, train_perplexity=52.077217
Batch 410, train_perplexity=64.449356
Batch 415, train_perplexity=56.494267
Batch 420, train_perplexity=64.04216
Batch 425, train_perplexity=57.251175
Batch 430, train_perplexity=57.695385
Batch 435, train_perplexity=54.236847
Batch 440, train_perplexity=56.8846
Batch 445, train_perplexity=57.837353
Batch 450, train_perplexity=58.246605
Batch 455, train_perplexity=57.106586
Batch 460, train_perplexity=64.358574
Batch 465, train_perplexity=56.807457
Batch 470, train_perplexity=53.64445
Batch 475, train_perplexity=74.91419
Batch 480, train_perplexity=53.4275
Batch 485, train_perplexity=56.270153
Batch 490, train_perplexity=60.796894
Batch 495, train_perplexity=55.42223
Batch 500, train_perplexity=62.55376
Batch 505, train_perplexity=59.23779
Batch 510, train_perplexity=54.147606
Batch 515, train_perplexity=50.51141
Batch 520, train_perplexity=58.89435
Batch 525, train_perplexity=64.62174
Batch 530, train_perplexity=57.563484
Batch 535, train_perplexity=51.50579
Batch 540, train_perplexity=52.09659
Batch 545, train_perplexity=54.018223
Batch 550, train_perplexity=51.5651
Batch 555, train_perplexity=58.457542
Batch 560, train_perplexity=54.137436
Batch 565, train_perplexity=50.531033
Batch 570, train_perplexity=55.08969
Batch 575, train_perplexity=50.7921
Batch 580, train_perplexity=42.35952
Batch 585, train_perplexity=52.725243
Batch 590, train_perplexity=46.800587
Batch 595, train_perplexity=48.291355
Batch 600, train_perplexity=45.43694
Batch 605, train_perplexity=47.53478
Batch 610, train_perplexity=52.942875
Batch 615, train_perplexity=56.741592
Batch 620, train_perplexity=49.05849
Batch 625, train_perplexity=44.43367
Batch 630, train_perplexity=47.4096
Batch 635, train_perplexity=52.854477
Batch 640, train_perplexity=44.87417
Batch 645, train_perplexity=47.966537
Batch 650, train_perplexity=61.292606
Batch 655, train_perplexity=51.794254
Batch 660, train_perplexity=51.999004
Batch 665, train_perplexity=42.53849
Batch 670, train_perplexity=46.456493
Batch 675, train_perplexity=49.495155
Batch 680, train_perplexity=47.479824
Batch 685, train_perplexity=44.426296
Batch 690, train_perplexity=47.06932
Batch 695, train_perplexity=43.55713
Batch 700, train_perplexity=49.31604
Batch 705, train_perplexity=44.407192
Batch 710, train_perplexity=49.277958
Batch 715, train_perplexity=48.801315
Batch 720, train_perplexity=42.450813
Batch 725, train_perplexity=57.959297
Batch 730, train_perplexity=50.1606
Batch 735, train_perplexity=50.022907
Batch 740, train_perplexity=47.557903
Batch 745, train_perplexity=48.842743
Batch 750, train_perplexity=45.03743
Batch 755, train_perplexity=51.566147
Batch 760, train_perplexity=50.192142
Batch 765, train_perplexity=42.058704
Batch 770, train_perplexity=44.17172
Batch 775, train_perplexity=39.392883
Batch 780, train_perplexity=42.42287
Batch 785, train_perplexity=40.59816
Batch 790, train_perplexity=51.70812
Batch 795, train_perplexity=44.204926
Batch 800, train_perplexity=45.463966
Batch 805, train_perplexity=44.58206
Batch 810, train_perplexity=49.05291
Batch 815, train_perplexity=41.855946
Batch 820, train_perplexity=44.59601
Batch 825, train_perplexity=45.05571
Batch 830, train_perplexity=41.779255
Batch 835, train_perplexity=43.206665
Batch 840, train_perplexity=36.575184
Batch 845, train_perplexity=42.04515
Batch 850, train_perplexity=45.520733
Batch 855, train_perplexity=41.972015
Batch 860, train_perplexity=43.398674
Batch 865, train_perplexity=40.38405
Batch 870, train_perplexity=42.955654
Batch 875, train_perplexity=41.73991
Batch 880, train_perplexity=42.991516
Batch 885, train_perplexity=44.682087
Batch 890, train_perplexity=39.433956
Batch 895, train_perplexity=39.440155
Batch 900, train_perplexity=40.052784
Batch 905, train_perplexity=51.934513
Batch 910, train_perplexity=44.44008
Batch 915, train_perplexity=36.106544
Batch 920, train_perplexity=42.731422
Batch 925, train_perplexity=40.659145
Batch 930, train_perplexity=42.580166
Batch 935, train_perplexity=42.868557
Batch 940, train_perplexity=40.923023
Batch 945, train_perplexity=43.29446
Batch 950, train_perplexity=42.754486
Batch 955, train_perplexity=44.344666
Batch 960, train_perplexity=38.73568
Batch 965, train_perplexity=34.231617
Batch 970, train_perplexity=42.061142
Batch 975, train_perplexity=45.934265
Batch 980, train_perplexity=44.82401
Batch 985, train_perplexity=46.189487
Batch 990, train_perplexity=52.2424
Batch 995, train_perplexity=40.673542
Batch 1000, train_perplexity=35.345043
Batch 1005, train_perplexity=39.61549
Batch 1010, train_perplexity=46.817684
Batch 1015, train_perplexity=40.847816
Batch 1020, train_perplexity=42.17972
Batch 1025, train_perplexity=43.353615
Batch 1030, train_perplexity=44.268757
Batch 1035, train_perplexity=43.57377
Batch 1040, train_perplexity=40.58068
Batch 1045, train_perplexity=49.571045
Batch 1050, train_perplexity=43.95976
Batch 1055, train_perplexity=46.664623
Batch 1060, train_perplexity=47.87783
Batch 1065, train_perplexity=42.4003
Batch 1070, train_perplexity=38.914917
Batch 1075, train_perplexity=45.891212
Batch 1080, train_perplexity=37.39583
Batch 1085, train_perplexity=40.040524
Batch 1090, train_perplexity=49.1075
Batch 1095, train_perplexity=48.66519
Batch 1100, train_perplexity=49.22263
Batch 1105, train_perplexity=45.85313
Batch 1110, train_perplexity=36.93852
Batch 1115, train_perplexity=40.747574
Batch 1120, train_perplexity=51.0569
Batch 1125, train_perplexity=43.64405
Batch 1130, train_perplexity=40.894005
Batch 1135, train_perplexity=39.980408
Batch 1140, train_perplexity=47.180664
Batch 1145, train_perplexity=43.742725
Batch 1150, train_perplexity=46.217083
Batch 1155, train_perplexity=43.43045
Batch 1160, train_perplexity=44.667942
Batch 1165, train_perplexity=42.14495
Batch 1170, train_perplexity=48.458443
Batch 1175, train_perplexity=40.97461
Batch 1180, train_perplexity=39.24008
Batch 1185, train_perplexity=41.384716
Batch 1190, train_perplexity=40.697456
Batch 1195, train_perplexity=42.657917
Batch 1200, train_perplexity=43.848843
Batch 1205, train_perplexity=38.43978
Batch 1210, train_perplexity=41.851715
Batch 1215, train_perplexity=39.8238
Batch 1220, train_perplexity=44.23046
Batch 1225, train_perplexity=44.817417
Batch 1230, train_perplexity=42.246033
Batch 1235, train_perplexity=46.591785
Batch 1240, train_perplexity=35.698162
Batch 1245, train_perplexity=48.47182
Batch 1250, train_perplexity=50.35996
Batch 1255, train_perplexity=43.559795
Batch 1260, train_perplexity=44.458317
Batch 1265, train_perplexity=39.134693
Batch 1270, train_perplexity=41.843243
Batch 1275, train_perplexity=41.830597
Batch 1280, train_perplexity=44.775143
Batch 1285, train_perplexity=36.67633
Batch 1290, train_perplexity=40.917023
Batch 1295, train_perplexity=45.89127
Batch 1300, train_perplexity=49.987213
Batch 1305, train_perplexity=43.49424
Batch 1310, train_perplexity=41.408836
Batch 1315, train_perplexity=39.229015
Batch 1320, train_perplexity=41.861164
Batch 1325, train_perplexity=46.02217
Batch 1330, train_perplexity=45.18174
Batch 1335, train_perplexity=41.07003
Batch 1340, train_perplexity=43.55124
Batch 1345, train_perplexity=43.76559
Batch 1350, train_perplexity=39.37037
Batch 1355, train_perplexity=39.54562
Batch 1360, train_perplexity=47.860188
Batch 1365, train_perplexity=41.975597
Batch 1370, train_perplexity=43.588387
Batch 1375, train_perplexity=44.04271
Batch 1380, train_perplexity=51.34684
Batch 1385, train_perplexity=44.062603
Batch 1390, train_perplexity=45.90659
Batch 1395, train_perplexity=45.378384
Batch 1400, train_perplexity=46.869072
Batch 1405, train_perplexity=45.38678
Batch 1410, train_perplexity=44.339878
Batch 1415, train_perplexity=45.908394
Batch 1420, train_perplexity=45.988224
Batch 1425, train_perplexity=41.180786
Batch 1430, train_perplexity=47.016357
Batch 1435, train_perplexity=47.876266
Batch 1440, train_perplexity=45.539787
Batch 1445, train_perplexity=49.36467
Batch 1450, train_perplexity=43.627853
Batch 1455, train_perplexity=43.93192
Batch 1460, train_perplexity=40.714954
Batch 1465, train_perplexity=37.569565
Batch 1470, train_perplexity=47.475395
Batch 1475, train_perplexity=44.768387
Batch 1480, train_perplexity=41.471912
Batch 1485, train_perplexity=46.552055
Batch 1490, train_perplexity=48.482906
Batch 1495, train_perplexity=46.886597
Batch 1500, train_perplexity=42.607178
Batch 1505, train_perplexity=49.202473
Batch 1510, train_perplexity=44.512283
Batch 1515, train_perplexity=43.758415
Batch 1520, train_perplexity=43.95941
Batch 1525, train_perplexity=47.272846
Batch 1530, train_perplexity=43.162804
Batch 1535, train_perplexity=39.540527
Batch 1540, train_perplexity=41.60447
Batch 1545, train_perplexity=53.207333
Batch 1550, train_perplexity=49.777946
Batch 1555, train_perplexity=45.016926
Batch 1560, train_perplexity=44.98077
Batch 1565, train_perplexity=46.480778
Batch 1570, train_perplexity=50.62987
Batch 1575, train_perplexity=47.44185
Batch 1580, train_perplexity=45.32941
Batch 1585, train_perplexity=50.98399
Batch 1590, train_perplexity=46.99389
Batch 1595, train_perplexity=48.69914
Batch 1600, train_perplexity=51.35258
Batch 1605, train_perplexity=48.409294
Batch 1610, train_perplexity=50.0226
Batch 1615, train_perplexity=50.897285
Batch 1620, train_perplexity=44.754128
Batch 1625, train_perplexity=50.5117
Batch 1630, train_perplexity=48.397213
Batch 1635, train_perplexity=49.31154
Batch 1640, train_perplexity=44.96636
Batch 1645, train_perplexity=46.677063
Batch 1650, train_perplexity=55.418953
Batch 1655, train_perplexity=56.68899
Batch 1660, train_perplexity=45.02211
Batch 1665, train_perplexity=46.603672
Batch 1670, train_perplexity=52.17933
Batch 1675, train_perplexity=52.390797
Batch 1680, train_perplexity=46.132698
Batch 1685, train_perplexity=51.396114
Batch 1690, train_perplexity=49.73794
Batch 1695, train_perplexity=45.734104
Batch 1700, train_perplexity=45.309723
Batch 1705, train_perplexity=46.225975
Batch 1710, train_perplexity=56.506336
Batch 1715, train_perplexity=53.49671
Batch 1720, train_perplexity=59.57011
Batch 1725, train_perplexity=50.683178
Batch 1730, train_perplexity=50.86794
Batch 1735, train_perplexity=47.332314
Batch 1740, train_perplexity=50.605515
Batch 1745, train_perplexity=56.297367
Batch 1750, train_perplexity=51.99831
Batch 1755, train_perplexity=47.504734
Batch 1760, train_perplexity=54.753876
Batch 1765, train_perplexity=47.197605
Batch 1770, train_perplexity=47.46091
Batch 1775, train_perplexity=48.773632
Batch 1780, train_perplexity=57.415836
Batch 1785, train_perplexity=56.998043
Batch 1790, train_perplexity=51.16837
Batch 1795, train_perplexity=44.49917
Batch 1800, train_perplexity=53.629524
Batch 1805, train_perplexity=51.234386
Batch 1810, train_perplexity=47.194378
Batch 1815, train_perplexity=59.82562
Batch 1820, train_perplexity=53.501534
Batch 1825, train_perplexity=50.08869
Batch 1830, train_perplexity=52.233074
Batch 1835, train_perplexity=46.046066
Batch 1840, train_perplexity=47.623863
Batch 1845, train_perplexity=52.890026
Batch 1850, train_perplexity=47.42173
Batch 1855, train_perplexity=47.73224
Batch 1860, train_perplexity=47.095203
Batch 1865, train_perplexity=45.650177
Batch 1870, train_perplexity=47.103996
Batch 1875, train_perplexity=46.80564
Batch 1880, train_perplexity=42.031788
Batch 1885, train_perplexity=46.285606
Batch 1890, train_perplexity=48.86723
Batch 1895, train_perplexity=43.885963
Batch 1900, train_perplexity=50.32538
Batch 1905, train_perplexity=47.83314
Batch 1910, train_perplexity=46.905178
Batch 1915, train_perplexity=47.908752
Batch 1920, train_perplexity=54.897583
Batch 1925, train_perplexity=48.211803
Batch 1930, train_perplexity=51.694942
Batch 1935, train_perplexity=46.771587
Batch 1940, train_perplexity=48.124825
Batch 1945, train_perplexity=49.392197
Batch 1950, train_perplexity=49.719234
Batch 1955, train_perplexity=48.45091
Batch 1960, train_perplexity=42.7451
Batch 1965, train_perplexity=50.506462
Batch 1970, train_perplexity=51.363346
Batch 1975, train_perplexity=46.77346
Batch 1980, train_perplexity=49.766068
Batch 1985, train_perplexity=47.47833
Batch 1990, train_perplexity=51.410194
Batch 1995, train_perplexity=46.72407
Batch 2000, train_perplexity=43.018944
Batch 2005, train_perplexity=48.54825
Batch 2010, train_perplexity=47.579407
Batch 2015, train_perplexity=48.938793
Batch 2020, train_perplexity=47.017242
Batch 2025, train_perplexity=42.332623
Batch 2030, train_perplexity=46.042202
Batch 2035, train_perplexity=51.382267
Batch 2040, train_perplexity=49.289356
Batch 2045, train_perplexity=47.61627
Batch 2050, train_perplexity=41.51839
Batch 2055, train_perplexity=45.677612
Batch 2060, train_perplexity=56.030468
Batch 2065, train_perplexity=47.358772
Batch 2070, train_perplexity=41.52032
Batch 2075, train_perplexity=50.325905
Batch 2080, train_perplexity=47.62311
Batch 2085, train_perplexity=51.895267
Batch 2090, train_perplexity=49.649982
Batch 2095, train_perplexity=54.9096
Batch 2100, train_perplexity=46.789845
Batch 2105, train_perplexity=47.125027
Batch 2110, train_perplexity=52.77882
Batch 2115, train_perplexity=46.775913
Batch 2120, train_perplexity=49.874718
Batch 2125, train_perplexity=47.826103
Batch 2130, train_perplexity=54.589417
Batch 2135, train_perplexity=52.46937
Batch 2140, train_perplexity=50.74326
Batch 2145, train_perplexity=46.593052
Batch 2150, train_perplexity=56.0781
Batch 2155, train_perplexity=51.050255
Batch 2160, train_perplexity=49.284634
Batch 2165, train_perplexity=49.77277
Batch 2170, train_perplexity=46.275345
Batch 2175, train_perplexity=45.39695
Batch 2180, train_perplexity=48.587524
Batch 2185, train_perplexity=46.226734
Batch 2190, train_perplexity=53.04877
Batch 2195, train_perplexity=53.550182
Batch 2200, train_perplexity=49.8765
Batch 2205, train_perplexity=49.657585
Batch 2210, train_perplexity=51.27838
Batch 2215, train_perplexity=54.61747
Batch 2220, train_perplexity=48.324112
Batch 2225, train_perplexity=50.826694
Batch 2230, train_perplexity=53.998302
Batch 2235, train_perplexity=51.160477
Batch 2240, train_perplexity=52.62353
Batch 2245, train_perplexity=48.830193
Batch 2250, train_perplexity=51.298862
Batch 2255, train_perplexity=47.49983
Batch 2260, train_perplexity=45.925407
Batch 2265, train_perplexity=45.96114
Batch 2270, train_perplexity=40.93414
Batch 2275, train_perplexity=43.324486
Batch 2280, train_perplexity=49.31154
Batch 2285, train_perplexity=51.393284
Batch 2290, train_perplexity=50.512806
Batch 2295, train_perplexity=54.526302
Batch 2300, train_perplexity=56.437023
Batch 2305, train_perplexity=48.55767
Batch 2310, train_perplexity=45.78761
Batch 2315, train_perplexity=49.355858
Batch 2320, train_perplexity=49.818623
Batch 2325, train_perplexity=42.72916
Batch 2330, train_perplexity=41.08361
Batch 2335, train_perplexity=49.012566
Batch 2340, train_perplexity=50.9384
Batch 2345, train_perplexity=50.964798
Batch 2350, train_perplexity=47.562317
Batch 2355, train_perplexity=43.15634
Batch 2360, train_perplexity=47.469997
Batch 2365, train_perplexity=47.249725
Batch 2370, train_perplexity=42.55227
Batch 2375, train_perplexity=46.492447
Batch 2380, train_perplexity=48.55936
Batch 2385, train_perplexity=45.547203
Batch 2390, train_perplexity=48.418484
Batch 2395, train_perplexity=38.78639
Batch 2400, train_perplexity=48.998337
Batch 2405, train_perplexity=47.334255
Batch 2410, train_perplexity=50.882908
Batch 2415, train_perplexity=44.877903
Batch 2420, train_perplexity=46.555176
Batch 2425, train_perplexity=39.64517
Batch 2430, train_perplexity=44.66443
Batch 2435, train_perplexity=48.378536
Batch 2440, train_perplexity=46.719727
Batch 2445, train_perplexity=49.564686
Batch 2450, train_perplexity=51.869835
Batch 2455, train_perplexity=49.39461
Batch 2460, train_perplexity=43.4602
Batch 2465, train_perplexity=45.233734
Batch 2470, train_perplexity=44.67166
Batch 2475, train_perplexity=46.032322
Batch 2480, train_perplexity=48.378754
Batch 2485, train_perplexity=40.884747
Batch 2490, train_perplexity=44.366398
Batch 2495, train_perplexity=42.169636
Batch 2500, train_perplexity=48.63094
Batch 2505, train_perplexity=41.669342
Batch 2510, train_perplexity=47.770683
Batch 2515, train_perplexity=51.37036
Batch 2520, train_perplexity=42.52912
Batch 2525, train_perplexity=40.22109
Batch 2530, train_perplexity=42.36833
Batch 2535, train_perplexity=47.37334
Batch 2540, train_perplexity=45.257328
Batch 2545, train_perplexity=47.202625
Batch 2550, train_perplexity=53.70379
Batch 2555, train_perplexity=51.1238
Batch 2560, train_perplexity=41.041096
Batch 2565, train_perplexity=43.88092
Batch 2570, train_perplexity=42.745483
Batch 2575, train_perplexity=41.572113
Batch 2580, train_perplexity=46.80505
Batch 2585, train_perplexity=46.307373
Batch 2590, train_perplexity=39.10316
Batch 2595, train_perplexity=42.70325
Batch 2600, train_perplexity=41.265545
Batch 2605, train_perplexity=43.23321
Batch 2610, train_perplexity=39.45885
Batch 2615, train_perplexity=54.28483
Batch 2620, train_perplexity=57.816784
Batch 2625, train_perplexity=52.637722
Batch 2630, train_perplexity=41.524387
Batch 2635, train_perplexity=50.79836
Batch 2640, train_perplexity=51.756454
Batch 2645, train_perplexity=46.38252
Batch 2650, train_perplexity=38.91081
Batch 2655, train_perplexity=42.89249
Batch 2660, train_perplexity=42.888485
Batch 2665, train_perplexity=39.530678
Batch 2670, train_perplexity=46.767715
Batch 2675, train_perplexity=50.23232
Batch 2680, train_perplexity=41.62851
Batch 2685, train_perplexity=44.270454
Batch 2690, train_perplexity=50.879387
Batch 2695, train_perplexity=44.71054
Batch 2700, train_perplexity=43.53574
Batch 2705, train_perplexity=47.322914
Batch 2710, train_perplexity=41.72581
Batch 2715, train_perplexity=39.216137
Batch 2720, train_perplexity=41.54065
Batch 2725, train_perplexity=42.546314
Batch 2730, train_perplexity=44.24686
Batch 2735, train_perplexity=39.42588
Batch 2740, train_perplexity=46.124077
Batch 2745, train_perplexity=42.73019
Batch 2750, train_perplexity=44.32972
Batch 2755, train_perplexity=47.931488
Batch 2760, train_perplexity=39.9611
Batch 2765, train_perplexity=40.020863
Batch 2770, train_perplexity=43.36231
Batch 2775, train_perplexity=46.291
Batch 2780, train_perplexity=45.99729
Batch 2785, train_perplexity=39.44416
Batch 2790, train_perplexity=42.08377
Batch 2795, train_perplexity=47.60715
Batch 2800, train_perplexity=40.51813
Batch 2805, train_perplexity=41.35051
Batch 2810, train_perplexity=35.837967
Batch 2815, train_perplexity=42.960754
Batch 2820, train_perplexity=45.102936
Batch 2825, train_perplexity=48.20669
Batch 2830, train_perplexity=40.31408
Batch 2835, train_perplexity=46.55158
Batch 2840, train_perplexity=40.330154
Batch 2845, train_perplexity=41.14831
Batch 2850, train_perplexity=41.820984
Batch 2855, train_perplexity=39.13667
Batch 2860, train_perplexity=42.033524
Batch 2865, train_perplexity=38.107365
Batch 2870, train_perplexity=34.441216
Batch 2875, train_perplexity=42.879242
Batch 2880, train_perplexity=51.705715
Batch 2885, train_perplexity=45.679497
Batch 2890, train_perplexity=40.646816
Batch 2895, train_perplexity=42.441242
Batch 2900, train_perplexity=41.986187
Batch 2905, train_perplexity=41.686115
Batch 2910, train_perplexity=43.386486
Batch 2915, train_perplexity=41.42582
Batch 2920, train_perplexity=41.87498
Batch 2925, train_perplexity=39.163395
Batch 2930, train_perplexity=44.760414
Batch 2935, train_perplexity=39.55142
Batch 2940, train_perplexity=41.2951
Batch 2945, train_perplexity=45.509827
Batch 2950, train_perplexity=37.82584
Batch 2955, train_perplexity=41.450638
Batch 2960, train_perplexity=36.70623
Batch 2965, train_perplexity=41.39182
Batch 2970, train_perplexity=38.663197
Batch 2975, train_perplexity=36.829098
Batch 2980, train_perplexity=40.743763
Batch 2985, train_perplexity=43.16116
Batch 2990, train_perplexity=41.275394
Batch 2995, train_perplexity=42.72356
Batch 3000, train_perplexity=40.51245
Batch 3005, train_perplexity=38.584545
Batch 3010, train_perplexity=41.92579
Batch 3015, train_perplexity=37.6883
Batch 3020, train_perplexity=39.649555
Batch 3025, train_perplexity=41.86001
Batch 3030, train_perplexity=42.989742
Batch 3035, train_perplexity=41.900948
Batch 3040, train_perplexity=39.11167
Batch 3045, train_perplexity=41.266373
Batch 3050, train_perplexity=36.288433
Batch 3055, train_perplexity=40.049145
Batch 3060, train_perplexity=40.840992
Batch 3065, train_perplexity=42.66106
Batch 3070, train_perplexity=38.21957
Batch 3075, train_perplexity=40.506165
Batch 3080, train_perplexity=39.3834
Batch 3085, train_perplexity=36.979393
Batch 3090, train_perplexity=40.506367
Batch 3095, train_perplexity=38.98331
Batch 3100, train_perplexity=35.319458
Batch 3105, train_perplexity=36.36784
Batch 3110, train_perplexity=38.641117
Batch 3115, train_perplexity=36.971294
Batch 3120, train_perplexity=40.02888
Batch 3125, train_perplexity=40.13334
Batch 3130, train_perplexity=37.025448
Batch 3135, train_perplexity=40.37293
Batch 3140, train_perplexity=39.49829
Batch 3145, train_perplexity=42.863613
Batch 3150, train_perplexity=41.24265
Batch 3155, train_perplexity=39.806625
Batch 3160, train_perplexity=37.343502
Batch 3165, train_perplexity=39.755344
Batch 3170, train_perplexity=35.47277
Batch 3175, train_perplexity=42.042725
Batch 3180, train_perplexity=37.221317
Batch 3185, train_perplexity=36.79785
Batch 3190, train_perplexity=36.418358
Batch 3195, train_perplexity=40.830975
Batch 3200, train_perplexity=36.657143
Batch 3205, train_perplexity=37.359833
Batch 3210, train_perplexity=38.2343
Batch 3215, train_perplexity=38.663086
Batch 3220, train_perplexity=39.599926
Batch 3225, train_perplexity=39.875103
Batch 3230, train_perplexity=42.3852
Batch 3235, train_perplexity=40.903515
Batch 3240, train_perplexity=41.184742
Batch 3245, train_perplexity=38.437492
Batch 3250, train_perplexity=35.717804
Batch 3255, train_perplexity=36.18408
Batch 3260, train_perplexity=33.30879
Batch 3265, train_perplexity=38.39494
Batch 3270, train_perplexity=37.384747
Batch 3275, train_perplexity=40.86203
Batch 3280, train_perplexity=46.190468
Batch 3285, train_perplexity=43.42482
Batch 3290, train_perplexity=39.978653
Batch 3295, train_perplexity=40.106983
Batch 3300, train_perplexity=36.91185
Batch 3305, train_perplexity=56.477837
Batch 3310, train_perplexity=38.290104
Batch 3315, train_perplexity=40.106594
Batch 3320, train_perplexity=40.410446
Batch 3325, train_perplexity=40.750233
Batch 3330, train_perplexity=36.647915
Batch 3335, train_perplexity=35.313103
Batch 3340, train_perplexity=35.392147
Batch 3345, train_perplexity=35.276608
Batch 3350, train_perplexity=44.80641
Batch 3355, train_perplexity=43.52503
Batch 3360, train_perplexity=42.300156
Batch 3365, train_perplexity=41.465
Batch 3370, train_perplexity=34.14863
Batch 3375, train_perplexity=40.732983
Batch 3380, train_perplexity=34.715412
Batch 3385, train_perplexity=36.7463
Batch 3390, train_perplexity=36.61023
Batch 3395, train_perplexity=33.377407
Batch 3400, train_perplexity=32.092167
Batch 3405, train_perplexity=35.142418
Batch 3410, train_perplexity=38.276558
Batch 3415, train_perplexity=32.995354
Batch 3420, train_perplexity=40.153244
Batch 3425, train_perplexity=37.387913
Batch 3430, train_perplexity=37.120632
Batch 3435, train_perplexity=38.36956
Batch 3440, train_perplexity=37.968727
Batch 3445, train_perplexity=37.699383
Batch 3450, train_perplexity=33.48945
Batch 3455, train_perplexity=34.06986
Batch 3460, train_perplexity=33.474213
Batch 3465, train_perplexity=38.61138
Batch 3470, train_perplexity=35.77053
Batch 3475, train_perplexity=35.79527
Batch 3480, train_perplexity=37.889763
Batch 3485, train_perplexity=32.90236
Batch 3490, train_perplexity=38.996204
Batch 3495, train_perplexity=37.403076
Batch 3500, train_perplexity=35.524143
Batch 3505, train_perplexity=37.283966
Batch 3510, train_perplexity=40.31182
Batch 3515, train_perplexity=30.932444
Batch 3520, train_perplexity=33.726395
Batch 3525, train_perplexity=41.694168
Batch 3530, train_perplexity=38.227745
Batch 3535, train_perplexity=38.93212
Batch 3540, train_perplexity=37.228725
Batch 3545, train_perplexity=35.602654
Batch 3550, train_perplexity=44.134327
Batch 3555, train_perplexity=36.883278
Batch 3560, train_perplexity=36.45167
Batch 3565, train_perplexity=37.564922
Batch 3570, train_perplexity=39.32382
Batch 3575, train_perplexity=33.663044
Batch 3580, train_perplexity=38.765327
Batch 3585, train_perplexity=38.728504
Batch 3590, train_perplexity=36.53904
Batch 3595, train_perplexity=37.07937
Batch 3600, train_perplexity=34.04075
Batch 3605, train_perplexity=37.119038
Batch 3610, train_perplexity=36.43854
Batch 3615, train_perplexity=34.23854
Batch 3620, train_perplexity=36.35859
Batch 3625, train_perplexity=40.590843
Batch 3630, train_perplexity=31.85196
Batch 3635, train_perplexity=34.542168
Batch 3640, train_perplexity=36.62136
Batch 3645, train_perplexity=39.49215
Batch 3650, train_perplexity=41.1822
Batch 3655, train_perplexity=40.497974
Batch 3660, train_perplexity=32.1466
Batch 3665, train_perplexity=43.23561
Batch 3670, train_perplexity=35.89592
Batch 3675, train_perplexity=35.414127
Batch 3680, train_perplexity=35.622597
Batch 3685, train_perplexity=36.513855
Batch 3690, train_perplexity=36.563477
Batch 3695, train_perplexity=30.940086
Batch 3700, train_perplexity=34.97822
Batch 3705, train_perplexity=36.34749
Batch 3710, train_perplexity=32.840008
Batch 3715, train_perplexity=33.961235
Batch 3720, train_perplexity=33.45736
Batch 3725, train_perplexity=35.37265
Batch 3730, train_perplexity=34.89517
Batch 3735, train_perplexity=34.44834
Batch 3740, train_perplexity=34.70189
Batch 3745, train_perplexity=33.27675
Batch 3750, train_perplexity=33.04533
Batch 3755, train_perplexity=35.0942
Batch 3760, train_perplexity=34.30066
Batch 3765, train_perplexity=33.226807
Batch 3770, train_perplexity=32.326736
Batch 3775, train_perplexity=39.107708
Batch 3780, train_perplexity=37.21945
Batch 3785, train_perplexity=35.949047
Batch 3790, train_perplexity=33.623756
Batch 3795, train_perplexity=35.737984
Batch 3800, train_perplexity=33.929153
Batch 3805, train_perplexity=33.682327
Batch 3810, train_perplexity=32.641243
Batch 3815, train_perplexity=31.360197
Batch 3820, train_perplexity=34.983116
Done training
